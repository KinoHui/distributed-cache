# 分布式缓存系统动态扩缩容和分区容错优化方案

## 1. 项目现状分析

### 当前架构
- 基于HTTP的节点间通信
- 硬编码节点地址（main.go:8001-8003）
- 使用一致性哈希算法进行数据分布
- 缺乏故障检测和自动恢复机制

### 需要优化的核心模块
1. **HTTPPool** (`jincache/http.go`) - 节点管理和通信
2. **ConsistentHash** (`jincache/consistenthash/`) - 数据分布算法
3. **Main** (`main.go`) - 服务启动和节点配置

## 2. 基于etcd的服务发现机制

### 2.1 etcd集成架构

```
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│ Cache Node  │    │ Cache Node  │    │ Cache Node  │
│   :8001     │    │   :8002     │    │   :8003     │
└─────┬───────┘    └─────┬───────┘    └─────┬───────┘
      │                  │                  │
      └──────────────────┼──────────────────┘
                         │
                ┌────────▼────────┐
                │      etcd       │
                │  Service Registry│
                └─────────────────┘
```

### 2.2 服务注册机制

#### 节点信息结构
```go
type NodeInfo struct {
    ID       string `json:"id"`        // 节点唯一标识
    Address  string `json:"address"`   // 节点访问地址
    Status   string `json:"status"`    // 节点状态：active/inactive
    LastSeen int64  `json:"last_seen"` // 最后心跳时间
}
```

#### etcd Key设计
- 服务注册路径：`/jincache/nodes/{node_id}`
- 心跳路径：`/jincache/heartbeat/{node_id}`
- 配置路径：`/jincache/config/{group_name}`

#### 服务注册流程
1. 节点启动时向etcd注册自身信息
2. 设置TTL（如30秒）的租约
3. 定期发送心跳续约
4. 节点下线时主动注销

### 2.3 服务发现实现

#### 新增模块：`jincache/discovery/`
```go
// discovery.go
type ServiceDiscovery struct {
    client   *clientv3.Client
    nodeID   string
    nodeAddr string
    leaseID  clientv3.LeaseID
    stopCh   chan struct{}
}

// 注册服务
func (sd *ServiceDiscovery) Register() error

// 发送心跳
func (sd *ServiceDiscovery) sendHeartbeat()

// 监听节点变化
func (sd *ServiceDiscovery) WatchNodes() chan []NodeInfo

// 注销服务
func (sd *ServiceDiscovery) Deregister() error
```

## 3. 分区容错和故障检测机制

### 3.1 故障检测策略

#### 多层次检测机制
1. **etcd租约检测** - 基础存活检测（30秒TTL）
2. **节点间心跳** - 快速故障检测（5秒间隔）
3. **请求超时** - 实时故障发现（3秒超时）

#### 故障类型分类
- **网络分区** - 部分节点无法通信
- **节点宕机** - 节点完全不可用
- **节点缓慢** - 响应超时但服务可用

### 3.2 故障转移机制

#### 自动故障转移流程
1. 检测到节点故障
2. 从一致性哈希环中移除故障节点
3. 重新计算数据分布
4. 将故障节点的数据迁移到其他节点
5. 更新路由表

#### 故障恢复机制
1. 节点重新上线后自动注册
2. 触发数据再平衡
3. 渐进式迁移数据回原节点

### 3.3 容错增强

#### 请求重试策略
```go
type RetryConfig struct {
    MaxRetries int           // 最大重试次数
    Backoff    time.Duration // 退避时间
    Multiplier float64       // 退避倍数
}
```

#### 熔断器模式
- 失败率超过阈值时熔断
- 定期尝试恢复
- 防止级联故障

## 4. 数据迁移和一致性哈希优化

### 4.1 动态一致性哈希

#### 当前问题
- 一致性哈希重建需要清空所有节点
- 数据迁移量过大

#### 优化方案
1. **支持动态添加/删除节点**
2. **虚拟节点优化** - 增加虚拟节点数量（50→150）
3. **渐进式数据迁移**

#### 优化后的ConsistentHash
```go
// 新增方法
func (m *Map) AddNode(key string)       // 动态添加节点
func (m *Map) RemoveNode(key string)    // 动态删除节点
func (m *Map) GetMigrationPlan(old, new Map) map[string]string // 生成迁移计划
```

### 4.2 数据迁移策略

#### 迁移触发条件
1. 节点加入/离开
2. 负载不均衡
3. 手动触发再平衡

#### 迁移流程
1. 计算需要迁移的key列表
2. 批量迁移数据（避免阻塞）
3. 更新路由信息
4. 验证迁移结果

#### 迁移实现
```go
// migration.go
type DataMigrator struct {
    localGroup *Group
    targetNode string
    batchSize  int
}

func (dm *DataMigrator) MigrateKeys(keys []string) error
func (dm *DataMigrator) VerifyMigration(keys []string) error
```

## 5. 具体实现方案

### 5.1 项目结构重组

```
jincache/
├── discovery/          # 服务发现模块
│   ├── discovery.go
│   └── watcher.go
├── fault_tolerance/    # 容错模块
│   ├── detector.go
│   ├── retry.go
│   └── circuit.go
├── migration/          # 数据迁移模块
│   ├── migrator.go
│   └── planner.go
├── consistenthash/
│   └── consistenthash.go # 优化版本
├── http.go             # 增强HTTPPool
├── jincache.go
└── ...
```

### 5.2 HTTPPool增强

#### 新增功能
1. 集成服务发现
2. 动态节点管理
3. 故障检测和转移
4. 负载均衡

#### 关键接口
```go
type EnhancedHTTPPool struct {
    *HTTPPool
    discovery   *ServiceDiscovery
    detector    *FaultDetector
    migrator    *DataMigrator
    nodeStatus  map[string]bool
}

// 动态更新节点列表
func (p *EnhancedHTTPPool) UpdateNodes(nodes []string)

// 获取健康节点
func (p *EnhancedHTTPPool) GetHealthyNodes() []string

// 处理节点故障
func (p *EnhancedHTTPPool) HandleNodeFailure(nodeAddr string)
```

### 5.3 启动流程优化

#### 新的启动流程
```go
func main() {
    // 1. 解析命令行参数
    // 2. 连接etcd
    // 3. 注册服务发现
    // 4. 启动故障检测器
    // 5. 启动数据迁移服务
    // 6. 启动HTTP服务
    // 7. 监听节点变化
}
```

## 6. 配置管理

### 6.1 配置文件结构
```yaml
# config.yaml
server:
  port: 8001
  api_port: 9999

etcd:
  endpoints:
    - "localhost:2379"
  dial_timeout: 5s
  lease_ttl: 30s

cache:
  group_name: "scores"
  cache_bytes: 2096
  lru_replicas: 150

fault_tolerance:
  heartbeat_interval: 5s
  request_timeout: 3s
  max_retries: 3
  circuit_threshold: 0.5

migration:
  batch_size: 100
  migration_timeout: 30s
```

### 6.2 动态配置更新
- 支持热更新配置
- 通过etcd分发配置变更
- 配置变更时触发相应调整

## 7. 实施计划

### 阶段一：基础服务发现（1-2周）
- [ ] 集成etcd客户端
- [ ] 实现服务注册和发现
- [ ] 基础心跳机制

### 阶段二：故障检测和转移（1-2周）
- [ ] 故障检测器实现
- [ ] 自动故障转移
- [ ] 重试和熔断机制

### 阶段三：数据迁移优化（1周）
- [ ] 动态一致性哈希
- [ ] 数据迁移实现
- [ ] 负载均衡优化

### 阶段四：集成测试和优化（1周）
- [ ] 端到端测试
- [ ] 性能调优
- [ ] 文档完善

## 8. 预期收益

### 可用性提升
- 节点故障自动恢复时间：从手动干预→30秒内
- 系统可用性：从单点故障→99.9%+

### 运维效率
- 节点扩缩容：从修改代码→一键操作
- 故障处理：从人工介入→自动恢复

### 扩展性
- 支持动态添加/移除节点
- 支持大规模部署（100+节点）
- 支持跨机房部署

## 9. 风险评估

### 技术风险
- etcd单点故障 → 部署etcd集群
- 数据一致性问题 → 加强测试和验证
- 性能影响 → 性能测试和优化

### 实施风险
- 改动范围大 → 分阶段实施
- 测试复杂度 → 完善测试用例
- 向后兼容 → 保持API兼容性

这个优化方案在保持系统简洁性的同时，显著提升了分布式缓存的可用性和可运维性。通过etcd作为服务发现中心，实现了真正的动态扩缩容和自动故障恢复。