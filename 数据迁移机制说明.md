# 数据迁移机制说明

## 概述

本文档描述了分布式缓存系统中实现的数据迁移机制，采用"先迁移，后清理"的三阶段策略，确保数据在节点变更时平滑迁移，避免缓存雪崩。

---

## 触发时机

当服务发现检测到节点列表变化时触发（`jincache/http.go:690`）：

```go
// handleNodeChange 处理节点变化
func (p *HTTPPool) handleNodeChange(nodes []discovery.NodeInfo) {
    // 更新节点列表和哈希环
    // ...
    
    // 触发数据迁移和清理
    if oldPeers != nil && len(activeAddrs) > 0 {
        go p.cleanupStaleData(oldPeers, p.peers)
    }
}
```

**触发条件**：
- 节点列表发生变化（新增或移除节点）
- 旧哈希环和新哈希环不同
- 异步执行，不阻塞主流程

---

## 三阶段迁移流程

### 第一阶段：识别需要迁移的 key

**代码位置**：`jincache/http.go:730-750`

```go
// 获取当前缓存中的所有 key
keys := p.localGroup.GetKeys()
log.Printf("[HTTPPool] Found %d keys in cache", len(keys))

// 识别需要迁移的 key
keysToMigrate := make(map[string]string) // key -> targetNode
for _, key := range keys {
    // 使用新哈希环判断 key 应该路由到哪个节点
    targetNode := newPeers.Get(key)
    
    // 如果 key 应该路由到其他节点，则需要迁移
    if targetNode != "" && targetNode != p.self {
        keysToMigrate[key] = targetNode
    }
}
```

**关键点**：
- 只检查本地缓存中存在的 key
- 使用**新哈希环**判断 key 的归属
- 记录需要迁移的 key 及其目标节点
- 如果 `keysToMigrate` 为空，直接返回

---

### 第二阶段：迁移数据到目标节点

**代码位置**：`jincache/http.go:752-790`

```go
migratedCount := 0
failedCount := 0

for key, targetNode := range keysToMigrate {
    // 直接从本地缓存获取数据（避免触发路由）
    view, ok := p.localGroup.mainCache.get(key)
    if !ok {
        log.Printf("[HTTPPool] Failed to get key %s from local cache (not found)", key)
        failedCount++
        continue
    }

    // 构造迁移请求
    req := &pb.Request{
        Group: p.localGroup.Name(),
        Key:   key,
        Value: view.ByteSlice(),
    }

    // 获取目标节点的 HTTP 客户端
    p.mu.Lock()
    client, exists := p.httpPeerClients[targetNode]
    p.mu.Unlock()

    if !exists {
        log.Printf("[HTTPPool] No HTTP client found for target node %s", targetNode)
        failedCount++
        continue
    }

    // 将数据设置到目标节点
    if err := client.Set(req); err != nil {
        log.Printf("[HTTPPool] Failed to migrate key %s to %s: %v", key, targetNode, err)
        failedCount++
        continue
    }

    log.Printf("[HTTPPool] Successfully migrated key %s to %s", key, targetNode)
    migratedCount++
}

log.Printf("[HTTPPool] Migration completed: %d succeeded, %d failed", migratedCount, failedCount)
```

**关键点**：
- **直接访问底层缓存** `mainCache.get()`，绕过路由逻辑
- 使用 **PUT 请求** 将数据发送到目标节点
- 迁移失败不影响其他 key 的处理
- 统计成功/失败数量
- 使用互斥锁保护 `httpPeerClients` 的访问

---

### 第三阶段：延迟清理已迁移的数据

**代码位置**：`jincache/http.go:792-810`

```go
// 只有成功迁移的数据才清理
if migratedCount > 0 {
    // 延迟 500ms，确保目标节点处理完成
    time.Sleep(500 * time.Millisecond)

    cleanedCount := 0
    for key, targetNode := range keysToMigrate {
        // 再次验证目标节点（防止节点列表再次变化）
        currentTarget := newPeers.Get(key)
        if currentTarget != "" && currentTarget != p.self && currentTarget == targetNode {
            log.Printf("[HTTPPool] Removing migrated key %s (now on %s)", key, targetNode)
            p.localGroup.Remove(key)
            cleanedCount++
        }
    }

    log.Printf("[HTTPPool] Cleanup completed, removed %d migrated keys", cleanedCount)
} else {
    log.Printf("[HTTPPool] No successful migrations, skipping cleanup to avoid data loss")
}
```

**关键点**：
- **失败保护**：只有成功迁移的数据才清理
- **延迟清理**：等待 500ms 后才清理本地缓存
- **双重验证**：清理时再次检查目标节点是否一致
- **避免数据丢失**：迁移失败的 key 保留在本地

---

## 核心设计要点

### 1. 避免路由循环

**问题**：使用 `Group.Get(key)` 会触发路由到目标节点

```go
// ❌ 错误：会触发路由
view, err := p.localGroup.Get(key)
// 这会调用一致性哈希，发现 key 应该在目标节点
// 然后转发请求到目标节点，导致 404 错误
```

**解决**：直接访问 `mainCache.get(key)`，绕过路由逻辑

```go
// ✅ 正确：直接读取本地缓存
view, ok := p.localGroup.mainCache.get(key)
// 直接从 LRU 缓存读取，不触发路由
```

### 2. 失败保护机制

- 迁移失败的 key **保留在本地缓存**
- 避免因网络问题或目标节点故障导致数据丢失
- 下次节点变更时会再次尝试迁移

**失败场景**：
- 目标节点未启动（连接拒绝）
- 网络超时
- 目标节点返回错误

### 3. 延迟清理策略

- 等待 500ms 后才清理本地缓存
- 确保目标节点已接收并处理数据
- 避免清理后立即请求导致缓存穿透

### 4. 目标节点验证

清理时再次验证目标节点：
- 防止迁移期间节点列表再次变化
- 确保清理的 key 确实应该在其他节点

```go
// 再次检查目标节点
currentTarget := newPeers.Get(key)
if currentTarget != "" && currentTarget != p.self && currentTarget == targetNode {
    // 只有目标节点一致时才清理
    p.localGroup.Remove(key)
}
```

---

## 迁移场景示例

### 场景 1：添加新节点

**初始状态**：
- 节点：[http://localhost:8001]
- 缓存：100 个 key

**添加节点 http://localhost:8002**：

```
[HTTPPool] Node list changed, updating. Current: [http://localhost:8001], New: [http://localhost:8001 http://localhost:8002]
[HTTPPool] Self node: http://localhost:8001
[HTTPPool] Starting migration and cleanup of stale data
[HTTPPool] Found 100 keys in cache
[HTTPPool] Found 30 keys that need migration
[HTTPPool] Successfully migrated key user:1 to http://localhost:8002
[HTTPPool] Successfully migrated key user:2 to http://localhost:8002
...
[HTTPPool] Migration completed: 30 succeeded, 0 failed
[HTTPPool] Cleanup completed, removed 30 migrated keys
```

**最终状态**：
- 节点 8001：保留 70 个 key
- 节点 8002：接收 30 个 key
- 数据分布均匀，无缓存雪崩

### 场景 2：移除节点

**初始状态**：
- 节点：[http://localhost:8001, http://localhost:8002]

**移除节点 http://localhost:8002**：

```
[HTTPPool] Node list changed, updating. Current: [http://localhost:8001 http://localhost:8002], New: [http://localhost:8001]
[HTTPPool] Self node: http://localhost:8001
[HTTPPool] Starting migration and cleanup of stale data
[HTTPPool] Found 70 keys in cache
[HTTPPool] Found 0 keys that need migration
[HTTPPool] No keys need migration
```

**最终状态**：
- 节点 8001：保留所有 70 个 key
- 无需迁移（8001 是唯一节点）

### 场景 3：迁移失败（目标节点未启动）

**场景**：添加节点 8002，但 8002 未启动

```
[HTTPPool] Starting migration and cleanup of stale data
[HTTPPool] Found 100 keys in cache
[HTTPPool] Found 30 keys that need migration
[HTTPPool] Failed to migrate key user:1 to http://localhost:8002: connection refused
[HTTPPool] Failed to migrate key user:2 to http://localhost:8002: connection refused
...
[HTTPPool] Migration completed: 0 succeeded, 30 failed
[HTTPPool] No successful migrations, skipping cleanup to avoid data loss
```

**最终状态**：
- 节点 8001：保留所有 100 个 key（包括应该迁移的 30 个）
- 数据未丢失，下次 8002 启动后会再次尝试迁移

---

## 日志输出说明

### 正常迁移日志

```
[HTTPPool] Starting migration and cleanup of stale data
[HTTPPool] Found 100 keys in cache
[HTTPPool] Found 30 keys that need migration
[HTTPPool] Successfully migrated key user:1 to http://localhost:8002
[HTTPPool] Successfully migrated key user:2 to http://localhost:8002
[HTTPPool] Migration completed: 30 succeeded, 0 failed
[HTTPPool] Cleanup completed, removed 30 migrated keys
```

### 迁移失败日志

```
[HTTPPool] Starting migration and cleanup of stale data
[HTTPPool] Found 100 keys in cache
[HTTPPool] Found 30 keys that need migration
[HTTPPool] Failed to migrate key user:1 to http://localhost:8002: dial tcp 127.0.0.1:8002: connect: connection refused
[HTTPPool] Migration completed: 0 succeeded, 30 failed
[HTTPPool] No successful migrations, skipping cleanup to avoid data loss
```

### 无需迁移日志

```
[HTTPPool] Starting migration and cleanup of stale data
[HTTPPool] Found 70 keys in cache
[HTTPPool] Found 0 keys that need migration
[HTTPPool] No keys need migration
```

---

## 优势对比

| 特性 | 修复前 | 修复后 |
|------|--------|--------|
| 迁移策略 | 直接删除 | 先迁移后清理 |
| 缓存可用性 | 雪崩风险 | 数据连续可用 |
| 数据丢失 | 可能丢失 | 失败保护 |
| 路由循环 | 可能触发 | 绕过路由 |
| 性能影响 | 穿透到数据源 | 平滑迁移 |
| 网络故障 | 数据丢失 | 保留本地数据 |

---

## 注意事项

### 1. 异步执行

- 迁移在 goroutine 中执行，不阻塞主流程
- 节点变更响应快速，迁移在后台进行

### 2. 幂等性

- 多次迁移同一 key 是安全的
- 目标节点会覆盖已有数据

### 3. 最终一致性

- 迁移期间可能有短暂的数据重复
- 清理完成后达到最终一致状态

### 4. 网络依赖

- 依赖节点间网络连通性
- 网络故障时保留本地数据，避免丢失

### 5. 性能影响

- 迁移期间会增加网络流量
- 延迟清理会增加短暂的内存占用
- 整体影响可控，避免了缓存雪崩

---

## 相关文件

- **核心实现**：`jincache/http.go` - `cleanupStaleData()` 方法
- **迁移器**：`jincache/migration/migrator.go`（保留用于未来扩展）
- **一致性哈希**：`jincache/consistenthash/consistenthash.go`
- **缓存实现**：`jincache/cache.go` - `mainCache.get()` 方法

---

## 测试建议

### 1. 添加节点测试

```bash
# 启动一个节点
./jincache -port=8001

# 添加数据
curl http://localhost:8001/_jincache/scores/key1

# 启动第二个节点
./jincache -port=8002

# 观察日志中的迁移过程
```

### 2. 移除节点测试

```bash
# 启动两个节点
./jincache -port=8001
./jincache -port=8002

# 停止第二个节点，观察迁移行为
```

### 3. 迁移失败测试

```bash
# 启动一个节点
./jincache -port=8001

# 添加数据
curl http://localhost:8001/_jincache/scores/key1

# 修改服务发现，添加未启动的节点
# 观察迁移失败后的保护机制
```

### 4. 并发测试

```bash
# 高并发场景下测试迁移的稳定性
# 验证数据一致性和性能影响
```

---

## 未来扩展

如果需要更强的迁移能力，可以考虑：

1. **批量迁移优化**：使用批量请求减少网络开销
2. **迁移进度跟踪**：记录迁移进度，支持断点续传
3. **迁移优先级**：热点数据优先迁移
4. **迁移限流**：控制迁移速度，避免影响正常请求
5. **迁移验证**：迁移后验证数据一致性
6. **迁移回滚**：迁移失败时回滚到原始状态